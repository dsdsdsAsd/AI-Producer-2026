"""
FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è Agentic RAG —Å–∏—Å—Ç–µ–º—ã.
Endpoints: auth, chat (SSE streaming), knowledge base upload, history.
"""

from fastapi import FastAPI, Depends, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from fastapi.staticfiles import StaticFiles
from sqlalchemy.orm import Session
from langchain_core.messages import HumanMessage
from pathlib import Path
import uuid
import json
import asyncio
from typing import AsyncGenerator, List

from config.settings import settings
from database.connection import get_db, get_vector_store
from database.repositories import UserRepository, ChatRepository, KnowledgeRepository
from database.models import User, UserChat, KnowledgeBase, BoardIdea, UserStrategy
from api.schemas import (
    LoginRequest, LoginResponse, ChatRequest, ChatHistoryResponse,
    UploadResponse, KnowledgeBaseStats, ErrorResponse, ChatMessage,
    EnhanceRequest, EnhanceResponse, TrendRequest, TrendResponse,
    IdeaCreate, IdeaUpdate, BoardIdeaResponse, StrategyUpdate, StrategyResponse
)
from api.dependencies import get_current_user, get_compiled_graph
from graph.graph import get_graph
from graph.state import create_initial_state
from utils.logger import logger
from utils.document_loader import load_document, is_supported_format
from utils.monitoring import get_langfuse_callback
from utils.chunking import chunk_document

# –°–æ–∑–¥–∞–µ–º FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
app = FastAPI(
    title="Agentic RAG System",
    description="–ú–Ω–æ–≥–æ–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ–±—â–µ–Ω–∏—è —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏",
    version="1.0.0"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.cors_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ú–æ–Ω—Ç–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–π–ª—ã (frontend)
frontend_path = Path(__file__).parent.parent.parent / "frontend"
if frontend_path.exists():
    app.mount("/static", StaticFiles(directory=str(frontend_path)), name="static")
    logger.info(f"Mounted frontend at /static from {frontend_path}")


# === Root Endpoint ===

@app.get("/")
async def root():
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ - —Ä–µ–¥–∏—Ä–µ–∫—Ç –Ω–∞ frontend"""
    from fastapi.responses import FileResponse
    
    index_path = frontend_path / "index.html"
    if index_path.exists():
        return FileResponse(index_path)
    else:
        return {"message": "Agentic RAG API", "docs": "/docs"}


# === Auth Endpoints ===

@app.post("/api/auth/login", response_model=LoginResponse)
async def login(
    request: LoginRequest,
    db: Session = Depends(get_db)
):
    """
    –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–ø—Ä–æ—Å—Ç–∞—è session-based).
    –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ç–æ–∫–µ–Ω.
    –ï—Å–ª–∏ –Ω–µ—Ç, —Å–æ–∑–¥–∞–µ—Ç –Ω–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
    """
    logger.info(f"Login attempt: {request.username}")
    
    user_repo = UserRepository(db)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å
    user = user_repo.get_by_username(request.username)
    
    if user:
        logger.info(f"Existing user: {user.username}")
    else:
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user = user_repo.create(request.username)
        logger.info(f"Created new user: {user.username}")
    
    return LoginResponse(
        user_id=str(user.id),
        username=user.username,
        session_token=user.session_token
    )


# === Board Ideas Endpoints ===

def map_board_idea(idea):
    return {
        "id": idea.id,
        "title": idea.title,
        "content": idea.content,
        "status": idea.status,
        "cover_type": idea.cover_type,
        "metadata": idea.extra_metadata,
        "created_at": idea.created_at,
        "updated_at": idea.updated_at
    }

@app.get("/planner/ideas", response_model=List[BoardIdeaResponse])
async def get_ideas(db: Session = Depends(get_db)):
    """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –∏–¥–µ–∏ —Å –¥–æ—Å–∫–∏"""
    ideas = db.query(BoardIdea).order_by(BoardIdea.created_at.desc()).all()
    return [map_board_idea(idea) for idea in ideas]

@app.post("/planner/ideas", response_model=BoardIdeaResponse)
async def create_idea(request: IdeaCreate, db: Session = Depends(get_db)):
    """–°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é –∏–¥–µ—é"""
    new_idea = BoardIdea(
        title=request.title,
        content=request.content,
        status=request.status,
        cover_type=request.cover_type,
        extra_metadata=request.metadata
    )
    db.add(new_idea)
    db.commit()
    db.refresh(new_idea)
    return map_board_idea(new_idea)

@app.patch("/planner/ideas/{idea_id}", response_model=BoardIdeaResponse)
async def update_idea(idea_id: uuid.UUID, request: IdeaUpdate, db: Session = Depends(get_db)):
    """–û–±–Ω–æ–≤–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∏–¥–µ—é"""
    idea = db.query(BoardIdea).filter(BoardIdea.id == idea_id).first()
    if not idea:
        raise HTTPException(status_code=404, detail="Idea not found")
    
    update_data = request.model_dump(exclude_unset=True)
    for key, value in update_data.items():
        if key == "metadata":
            idea.extra_metadata = value
        else:
            setattr(idea, key, value)
    
    db.commit()
    db.refresh(idea)
    return map_board_idea(idea)

@app.delete("/planner/ideas/{idea_id}")
async def delete_idea(idea_id: uuid.UUID, db: Session = Depends(get_db)):
    """–£–¥–∞–ª–∏—Ç—å –∏–¥–µ—é"""
    idea = db.query(BoardIdea).filter(BoardIdea.id == idea_id).first()
    if not idea:
        raise HTTPException(status_code=404, detail="Idea not found")
    
    db.delete(idea)
    db.commit()
    return {"status": "success", "message": "Idea deleted"}


# === User Strategy Endpoints ===

@app.get("/planner/strategy", response_model=StrategyResponse)
async def get_strategy(db: Session = Depends(get_db)):
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–ø–æ–∫–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ñ–∏–ª—å –Ω–∞ –≤—Å—é —Å–∏—Å—Ç–µ–º—É)"""
    strategy = db.query(UserStrategy).first()
    if not strategy:
        # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        strategy = UserStrategy(user_id="default")
        db.add(strategy)
        db.commit()
        db.refresh(strategy)
    return strategy

@app.post("/planner/strategy", response_model=StrategyResponse)
async def update_strategy(request: StrategyUpdate, db: Session = Depends(get_db)):
    """–û–±–Ω–æ–≤–∏—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏—é"""
    strategy = db.query(UserStrategy).first()
    if not strategy:
        strategy = UserStrategy(user_id="default")
        db.add(strategy)
    
    update_data = request.model_dump(exclude_unset=True)
    for key, value in update_data.items():
        setattr(strategy, key, value)
    
    db.commit()
    db.refresh(strategy)
    return strategy


# === Chat Endpoints ===

@app.post("/api/chat/stream")
async def chat_stream(
    request: ChatRequest,
    user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    –û—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –∏ –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ SSE streaming.
    """
    logger.info(f"Chat request from {user.username}: {request.message[:50]}...")
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º thread_id –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω
    thread_id = request.thread_id or str(uuid.uuid4())
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    chat_repo = ChatRepository(db)
    chat_repo.add_message(
        user_id=str(user.id),
        thread_id=thread_id,
        role="user",
        content=request.message
    )
    
    # –°–æ–∑–¥–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è SSE
    async def generate() -> AsyncGenerator[str, None]:
        full_answer = ""
        try:
            logger.info("üöÄ Starting chat with RAG system")
            
            # –ü–æ–ª—É—á–∞–µ–º –≥—Ä–∞—Ñ
            logger.info("üìä Loading LangGraph...")
            graph = get_graph()
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞
            history = chat_repo.get_history(str(user.id), thread_id, limit=10)
            messages = []
            for msg in history:
                if msg.role == "user":
                    messages.append(HumanMessage(content=msg.content))
                elif msg.role == "assistant":
                    messages.append(AIMessage(content=msg.content))
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
            messages.append(HumanMessage(content=request.message))
            logger.info(f"üìù Loaded {len(messages)} messages from history")
            
            # –°–æ–∑–¥–∞–µ–º –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            from graph.state import GraphState
            initial_state = GraphState(
                messages=messages,
                user_id=str(user.id),
                thread_id=thread_id
            )
            
            # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è checkpointer
            config = {
                "configurable": {
                    "thread_id": thread_id
                }
            }
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –≥—Ä–∞—Ñ
            logger.info("üîÑ Invoking graph...")
            final_state = graph.invoke(initial_state, config)
            logger.info("‚úÖ Graph completed successfully")
            
            # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç
            answer_messages = final_state.get("messages", [])
            if answer_messages:
                last_message = answer_messages[-1]
                full_answer = last_message.content if hasattr(last_message, 'content') else str(last_message)
            else:
                full_answer = "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç."
            
            logger.info(f"üí¨ Answer generated: {full_answer[:100]}...")
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            sources = final_state.get("sources", [])
            if sources:
                logger.info(f"üìö Found {len(sources)} sources")
                sources_data = json.dumps({
                    "type": "sources",
                    "sources": sources
                }, ensure_ascii=False)
                yield f"data: {sources_data}\n\n"
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç —á–∞—Å—Ç—è–º–∏ (—ç—Ñ—Ñ–µ–∫—Ç –ø–µ—á–∞—Ç–∏)
            words = full_answer.split()
            for i, word in enumerate(words):
                chunk_data = json.dumps({
                    "type": "token",
                    "content": word + " "
                }, ensure_ascii=False)
                yield f"data: {chunk_data}\n\n"
                
                # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                if i % 5 == 0:
                    await asyncio.sleep(0.05)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–∏–≥–Ω–∞–ª –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            done_data = json.dumps({
                "type": "done",
                "metadata": {
                    "sources_count": len(sources)
                }
            }, ensure_ascii=False)
            yield f"data: {done_data}\n\n"
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–≤–µ—Ç –≤ –ë–î
            chat_repo.add_message(
                user_id=str(user.id),
                thread_id=thread_id,
                role="assistant",
                content=full_answer,
                metadata={
                    "sources": sources
                }
            )
            logger.info("üíæ Answer saved to database")
            
        except Exception as e:
            logger.error(f"‚ùå Chat stream error: {e}", exc_info=True)
            error_data = json.dumps({
                "type": "error",
                "content": f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}"
            }, ensure_ascii=False)
            yield f"data: {error_data}\n\n"
            
            logger.info("Chat response completed")
            
        except Exception as e:
            logger.error(f"Chat error: {e}")
            error_data = json.dumps({
                "type": "error",
                "content": str(e)
            })
            yield f"data: {error_data}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )


# –¢–µ—Å—Ç–æ–≤—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç –ë–ï–ó –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ RAG
@app.post("/api/test-rag")
async def test_rag(request: ChatRequest):
    """
    –¢–µ—Å—Ç–æ–≤—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ RAG –±–µ–∑ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏.
    """
    logger.info(f"üß™ Test RAG request: {request.message[:50]}...")
    
    async def generate() -> AsyncGenerator[str, None]:
        try:
            logger.info("üöÄ Starting RAG test")
            
            # –ü–æ–ª—É—á–∞–µ–º –≥—Ä–∞—Ñ
            graph = get_graph()
            logger.info("üìä Graph loaded")
            
            # Langfuse callback –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ (–í–†–ï–ú–ï–ù–ù–û –û–¢–ö–õ–Æ–ß–ï–ù - –ø—Ä–æ–±–ª–µ–º—ã —Å API)
            # langfuse_cb = get_langfuse_callback(user_id="test_user", thread_id="test_thread")
            langfuse_cb = None
            
            # –ü—Ä–æ—Å—Ç–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –±–µ–∑ –∏—Å—Ç–æ—Ä–∏–∏
            from graph.state import GraphState
            initial_state = GraphState(
                messages=[HumanMessage(content=request.message)],
                user_id="test_user",
                thread_id="test_thread"
            )
            
            # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å Langfuse
            config = {
                "configurable": {"thread_id": "test"},
                "callbacks": [langfuse_cb] if langfuse_cb else []
            }
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –≥—Ä–∞—Ñ
            logger.info("üîÑ Invoking graph...")
            final_state = graph.invoke(initial_state, config)
            logger.info("‚úÖ Graph completed")
            
            # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç
            answer_messages = final_state.get("messages", [])
            if answer_messages:
                last_message = answer_messages[-1]
                full_answer = last_message.content if hasattr(last_message, 'content') else str(last_message)
            else:
                full_answer = "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç."
            
            logger.info(f"üí¨ Answer: {full_answer[:100]}...")
            
            # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            sources = final_state.get("sources", [])
            context = final_state.get("context", "")
            
            logger.info("=" * 80)
            logger.info("üìä RAG QUALITY REPORT:")
            logger.info(f"   Sources found: {len(sources)}")
            logger.info(f"   Context length: {len(context)} chars")
            
            if sources:
                scores = [s.get('similarity', 0.0) for s in sources if isinstance(s, dict)]
                if scores:
                    avg_score = sum(scores) / len(scores)
                    logger.info(f"   Avg similarity: {avg_score:.3f}")
                    logger.info(f"   Score range: {min(scores):.3f} - {max(scores):.3f}")
                    
                    logger.info("\n   üìÑ Top sources:")
                    for i, source in enumerate(sources[:5], 1):
                        if isinstance(source, dict):
                            sim = source.get('similarity', 0.0)
                            title = source.get('title', source.get('chapter', 'unknown'))
                            content_preview = source.get('content', '')[:100]
                            logger.info(f"      {i}. [{sim:.3f}] {title}")
                            logger.info(f"         Preview: {content_preview}...")
                else:
                    logger.warning("   ‚ö†Ô∏è No similarity scores found in sources")
            else:
                logger.warning("   ‚ö†Ô∏è No sources returned by RAG!")
            
            logger.info("=" * 80)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç
            words = full_answer.split()
            for word in words:
                chunk = json.dumps({"type": "token", "content": word + " "}, ensure_ascii=False)
                yield f"data: {chunk}\n\n"
                await asyncio.sleep(0.03)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
            metrics = {
                "type": "done",
                "sources_count": len(sources),
                "avg_similarity": sum([s.get('similarity', 0.0) for s in sources if isinstance(s, dict)]) / len(sources) if sources else 0.0
            }
            yield f"data: {json.dumps(metrics, ensure_ascii=False)}\n\n"
            
        except Exception as e:
            logger.error(f"‚ùå Test RAG error: {e}", exc_info=True)
            error = json.dumps({"type": "error", "content": str(e)}, ensure_ascii=False)
            yield f"data: {error}\n\n"
    
    return StreamingResponse(generate(), media_type="text/event-stream")



@app.get("/api/chat/threads")
async def get_user_threads(
    user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ —Ç—Ä–µ–¥—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ –¥–∞—Ç–µ"""
    chat_repo = ChatRepository(db)
    threads = chat_repo.get_all_threads(str(user.id))
    return {"threads": threads}


@app.get("/api/chat/history", response_model=ChatHistoryResponse)
async def get_chat_history(
    thread_id: str,
    limit: int = 50,
    user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """–ü–æ–ª—É—á–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞"""
    logger.info(f"Get history: user={user.username}, thread={thread_id}")
    
    chat_repo = ChatRepository(db)
    history = chat_repo.get_history(str(user.id), thread_id, limit)
    
    messages = [
        ChatMessage(
            role=msg.role,
            content=msg.content,
            created_at=msg.created_at,
            metadata=msg.extra_metadata
        )
        for msg in history
    ]
    
    return ChatHistoryResponse(
        messages=messages,
        total=len(messages),
        thread_id=thread_id
    )


# === Knowledge Base Endpoints ===

@app.post("/api/knowledge/upload", response_model=UploadResponse)
async def upload_document(
    file: UploadFile = File(...),
    user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
    logger.info(f"Upload document: {file.filename} by {user.username}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞
    if not is_supported_format(file.filename):
        raise HTTPException(
            status_code=400,
            detail=f"Unsupported file format. Supported: PDF, DOCX, TXT"
        )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
    upload_dir = Path("data/documents")
    upload_dir.mkdir(parents=True, exist_ok=True)
    
    file_path = upload_dir / file.filename
    
    with open(file_path, "wb") as f:
        content = await file.read()
        f.write(content)
    
    logger.info(f"File saved: {file_path}")
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –ø–∞—Ä—Å–∏–º –¥–æ–∫—É–º–µ–Ω—Ç
        text, file_type = load_document(file_path)
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
        chunks = chunk_document(text, file.filename)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º–∏ –±–∞—Ç—á–∞–º–∏ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
        import asyncio
        from functools import partial
        
        vector_store = get_vector_store()
        
        batch_size = 50
        texts = [chunk["content"] for chunk in chunks]
        metadatas = [chunk["metadata"] for chunk in chunks]
        
        tasks = []
        loop = asyncio.get_event_loop()
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            batch_metadatas = metadatas[i:i + batch_size]
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º to_thread (Python 3.9+) –∏–ª–∏ run_in_executor –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç–∏
            tasks.append(asyncio.to_thread(vector_store.add_texts, batch_texts, batch_metadatas))
            logger.info(f"Scheduled batch {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1}")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –±–∞—Ç—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        await asyncio.gather(*tasks)
        
        logger.info(f"Successfully added {len(chunks)} chunks to vector store total (via parallel processing)")
        
        return UploadResponse(
            filename=file.filename,
            chunks_count=len(chunks),
            message=f"Document uploaded successfully: {len(chunks)} chunks created and indexed"
        )
        
    except Exception as e:
        logger.error(f"Upload error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/knowledge/graph")
async def get_knowledge_graph(
    limit: int = 100,
    user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    –ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è 3D –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —É–∑–ª—ã (chunks) –∏ —Å–≤—è–∑–∏ (links).
    """
    logger.info(f"Fetching knowledge graph for {user.username}")
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —á–∞–Ω–∫–∏
    from database.connection import engine
    from sqlalchemy import text
    
    sql = text("""
        SELECT id, content, metadata
        FROM knowledge_base
        WHERE metadata->>'author' = 'Nikolay Velizhanin'
        ORDER BY created_at DESC
        LIMIT :limit
    """)
    
    nodes = []
    links = []
    
    try:
        with engine.connect() as conn:
            result = conn.execute(sql, {"limit": limit})
            rows = result.fetchall()
            
            for row in rows:
                metadata = row[2] or {}
                nodes.append({
                    "id": str(row[0]),
                    "name": metadata.get("source", "Chunk"),
                    "val": 1,
                    "content": row[1][:200] + "...",
                    "color": "#4f46e5" if "vtt" in metadata.get("source", "") else "#10b981"
                })
            
            # –°–æ–∑–¥–∞–µ–º —Å–≤—è–∑–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—â–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (source)
            # –í –±—É–¥—É—â–µ–º –∑–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Å–≤—è–∑–∏ –ø–æ —Å–µ–º–∞–Ω—Ç–∏–∫–µ
            for i in range(len(nodes)):
                for j in range(i + 1, len(nodes)):
                    if nodes[i]["name"] == nodes[j]["name"]:
                        links.append({
                            "source": nodes[i]["id"],
                            "target": nodes[j]["id"]
                        })
                        
    except Exception as e:
        logger.error(f"Graph fetch error: {e}")
        raise HTTPException(status_code=500, detail=str(e))
        
    return {"nodes": nodes, "links": links}


@app.get("/api/knowledge/stats", response_model=KnowledgeBaseStats)
async def get_knowledge_stats(
    user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
    knowledge_repo = KnowledgeRepository(db)
    
    total_chunks = knowledge_repo.count_chunks()
    sources = knowledge_repo.get_sources_list()
    
    documents = []
    for source in sources:
        chunks = knowledge_repo.get_by_source(source)
        documents.append({
            "source": source,
            "chunks_count": len(chunks),
            "created_at": chunks[0].created_at if chunks else None
        })
    
    return KnowledgeBaseStats(
        total_chunks=total_chunks,
        total_documents=len(documents),
        documents=documents
    )


# === Planner Endpoints ===

@app.post("/api/enhance-idea", response_model=EnhanceResponse)
async def enhance_idea(
    request: EnhanceRequest,
    user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    –£–ª—É—á—à–∏—Ç—å –∏–¥–µ—é —Å –ø–æ–º–æ—â—å—é AI.
    –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —Å—ã—Ä—É—é –∏–¥–µ—é –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä—É Hook-Value-CTA.
    """
    logger.info(f"Enhance idea for {user.username}: {request.title}")

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º LangGraph –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å RAG
    try:
        from langchain_openai import ChatOpenAI
        from langchain_core.output_parsers import JsonOutputParser
        from api.dependencies import get_compiled_graph
        from graph.state import create_initial_state
        
        graph = get_compiled_graph()
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –≥—Ä–∞—Ñ–∞
        prompt_text = f"""–£–ª—É—á—à–∏ —ç—Ç—É –∏–¥–µ—é:
        –ù–∞–∑–≤–∞–Ω–∏–µ: {request.title}
        –ö–æ–Ω—Ç–µ–Ω—Ç: {request.content}
        –§–æ–∫—É—Å: {request.focus}"""
        
        # –°–æ–∑–¥–∞–µ–º –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å —É–∫–∞–∑–∞–Ω–Ω–æ–π –ø–µ—Ä—Å–æ–Ω–æ–π
        initial_state = create_initial_state(
            user_id=str(user.id),
            thread_id=f"enhance_{uuid.uuid4()}",
            messages=[HumanMessage(content=prompt_text)]
        )
        initial_state["persona"] = request.persona or "velizhanin"
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º
        langfuse_cb = get_langfuse_callback(user_id=str(user.id), thread_id=initial_state["thread_id"])
        config = {
            "configurable": {"thread_id": initial_state["thread_id"]},
            "callbacks": [langfuse_cb] if langfuse_cb else []
        }
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≥—Ä–∞—Ñ
        logger.info(f"Invoking RAG graph (persona: {initial_state['persona']}) with monitoring")
        final_state = graph.invoke(initial_state, config)
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç
        answer_messages = final_state.get("messages", [])
        answer_text = answer_messages[-1].content if answer_messages else ""
        
        # –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –Ω–µ –≤ JSON (–∞–≥–µ–Ω—Ç –º–æ–≥ –≤—ã–¥–∞—Ç—å —Ç–µ–∫—Å—Ç), –ø—Ä–æ—Å–∏–º LLM –ø—Ä–∏–≤–µ—Å—Ç–∏ –µ–≥–æ –∫ —Å—Ö–µ–º–µ
        # –ù–æ –¥–ª—è –Ω–∞—á–∞–ª–∞ –ø–æ–ø—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –Ω–∞–ø—Ä—è–º—É—é
        try:
            import re
            json_match = re.search(r"\{.*\}", answer_text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(0))
        except:
            pass
            
        # Fallback: –ü–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ –ø—Ä–æ–º–ø—Ç (–µ—Å–ª–∏ –∞–≥–µ–Ω—Ç –≤—ã–¥–∞–ª –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç)
        from api.dependencies import get_llm
        parser = JsonOutputParser(pydantic_object=EnhanceResponse)
        llm = get_llm(temperature=0)
        
        correction_prompt = ChatPromptTemplate.from_messages([
            ("system", "Convert the following AI assistant response into a valid JSON object according to the schema."),
            ("user", "{text}\n\n{format_instructions}")
        ])
        
        correction_chain = correction_prompt | llm | parser
        return correction_chain.invoke({
            "text": answer_text,
            "format_instructions": parser.get_format_instructions()
        })
        
    except Exception as e:
        logger.error(f"Enhance error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/chat/stream")
async def chat_stream(
    request: ChatRequest,
    user: User = Depends(get_current_user)
):
    """
    –°—Ç—Ä–∏–º–∏–Ω–≥ —á–∞—Ç–∞ —á–µ—Ä–µ–∑ LangGraph.
    """
    logger.info(f"Chat stream for {user.username}: {request.message[:50]}...")
    
    from api.dependencies import get_compiled_graph
    from graph.state import create_initial_state
    
    graph = get_compiled_graph()
    
    initial_state = create_initial_state(
        user_id=str(user.id),
        thread_id=request.thread_id or f"chat_{uuid.uuid4()}",
        messages=[HumanMessage(content=request.message)]
    )
    
    config = {"configurable": {"thread_id": initial_state["thread_id"]}}
    
    async def event_generator():
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º astream –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ–±—ã—Ç–∏–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
            async for event in graph.astream(initial_state, config, stream_mode="messages"):
                # event - —ç—Ç–æ –∫–æ—Ä—Ç–µ–∂ (message, metadata) –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –≤–µ—Ä—Å–∏—è—Ö –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ message
                # –í —Ç–µ–∫—É—â–µ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ LangGraph —É–ø—Ä–æ—Å—Ç–∏–º –¥–æ —Ç–æ–∫–µ–Ω–æ–≤
                if hasattr(event[0], "content") and event[0].content:
                    chunk = {"type": "token", "content": event[0].content}
                    yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
            
            yield f"data: {json.dumps({'type': 'done'})}\n\n"
        except Exception as e:
            logger.error(f"Stream error: {e}")
            yield f"data: {json.dumps({'type': 'error', 'content': str(e)})}\n\n"

    return StreamingResponse(event_generator(), media_type="text/event-stream")


@app.post("/api/trend-ideas", response_model=TrendResponse)
async def generate_trend_ideas(
    request: TrendRequest,
    user: User = Depends(get_current_user)
):
    """
    –ù–∞–π—Ç–∏ —Ç—Ä–µ–Ω–¥–æ–≤—ã–µ –∏–¥–µ–∏ —á–µ—Ä–µ–∑ –≤–µ–±-–ø–æ–∏—Å–∫ –∏ –∑–Ω–∞–Ω–∏—è –í–µ–ª–∏–∂–∞–Ω–∏–Ω–∞.
    """
    logger.info(f"Trend scouting for {user.username} (Topic: {request.topic})")
    
    from api.dependencies import search_web_tool, get_llm
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import JsonOutputParser
    
    # 1. –ü–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
    topic_query = request.topic or "viral youtube shorts trends 2025"
    try:
        search_results = search_web_tool(f"trending YouTube Shorts topics 2025 {topic_query}")
    except Exception as e:
        logger.warning(f"Search tool failed: {e}. Using LLM internal knowledge.")
        search_results = "Data unavailable, use your internal knowledge about 2025 trends."
    
    try:
        # 2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–¥–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–∏—Å–∫–∞
        llm = get_llm(temperature=0.8)
        
        parser = JsonOutputParser(pydantic_object=TrendResponse)
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", """–¢—ã - —Ç—Ä–µ–Ω–¥-–∞–Ω–∞–ª–∏—Ç–∏–∫ –∏ –ò–ò-–ü—Ä–æ–¥—é—Å–µ—Ä. 
–¢–≤–æ—è –∑–∞–¥–∞—á–∞: –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–≤–µ–∂–∏—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–∏—Å–∫–∞ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å 3 —Å–∞–º—ã—Ö –≥–æ—Ä—è—á–∏—Ö –∏ –≤–∏—Ä–∞–ª—å–Ω—ã—Ö —Ç–µ–º—ã –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤–∏–¥–µ–æ (Shorts/Reels).
–¢–µ–º—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ —Å—Ç–∏–ª–µ 'Bento' –∏–ª–∏ 'Nikolay Velizhanin' - —á–µ—Ç–∫–∏–µ, –±—å—é—â–∏–µ –≤ –±–æ–ª–∏ –∏ –æ–±–µ—â–∞—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç.

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: JSON —Å–æ —Å–ø–∏—Å–∫–æ–º 'ideas', –≥–¥–µ —É –∫–∞–∂–¥–æ–π –∏–¥–µ–∏ –µ—Å—Ç—å 'title' (—è—Ä–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫) –∏ 'description' (–∫—Ä–∞—Ç–∫–∞—è —Å—É—Ç—å).
{format_instructions}"""),
            ("user", "–¢–µ–º–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {topic}\n–î–∞–Ω–Ω—ã–µ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞: {search_results}")
        ])
        
        chain = prompt | llm | parser
        return chain.invoke({
            "topic": topic_query,
            "search_results": search_results,
            "format_instructions": parser.get_format_instructions()
        })
        
    except Exception as e:
        logger.error(f"Trend generation error: {e}")
        # Fallback –Ω–∞ —Å—Ç–∞—Ä—É—é –ª–æ–≥–∏–∫—É –µ—Å–ª–∏ LLM/Parser —É–ø–∞–ª
        return {
            "ideas": [
                {"title": f"–¢—Ä–µ–Ω–¥: {topic_query} –≤ 2025", "description": "–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ò–ò –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —ç—Ç–æ–π —Å—Ñ–µ—Ä—ã."},
                {"title": "–°–µ–∫—Ä–µ—Ç –≤–∏—Ä–∞–ª—å–Ω–æ—Å—Ç–∏ –≤ –Ω–∏—à–µ {topic_query}", "description": "–†–∞–∑–±–æ—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–æ–µ –Ω–∞–±–∏—Ä–∞–µ—Ç 100–∫+ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤."},
                {"title": "–û—à–∏–±–∫–∞ –Ω–æ–≤–∏—á–∫–æ–≤ –≤ {topic_query}", "description": "–ü–æ—á–µ–º—É –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –∑–∞—Ö–æ–¥–∏—Ç –∏ –∫–∞–∫ —ç—Ç–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å –∑–∞ 15 —Å–µ–∫—É–Ω–¥."}
            ]
        }
    except Exception as e:
        logger.error(f"Trend error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# === Health Check ===

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    return {
        "status": "healthy",
        "environment": settings.environment,
        "llm_model": settings.llm_model
    }


# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
if __name__ == "__main__":
    import uvicorn
    
    uvicorn.run(
        "api.main:app",
        host=settings.api_host,
        port=settings.api_port,
        reload=settings.environment == "development"
    )
